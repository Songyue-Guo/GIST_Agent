#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”è¯„ä¼°è„šæœ¬ï¼šæ¯”è¾ƒåŸç‰ˆè¯„ä¼°vsåŸºäºCoTä¸Šä¸‹æ–‡çš„è¯„ä¼°
"""

import json
import os
import re
import time
import pandas as pd
import random
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging
from dataclasses import dataclass
from openai import AzureOpenAI

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from run_evaluation import DiagnosisEvaluator, AzureModel, EvaluationResult
from run_evaluation_with_cot import CoTDiagnosisEvaluator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComparisonEvaluator:
    """è¯„ä¼°å¯¹æ¯”å™¨"""
    
    def __init__(self, data_dir: str = "../data/train_data"):
        self.data_dir = data_dir
        self.original_evaluator = DiagnosisEvaluator(data_dir)
        self.cot_evaluator = CoTDiagnosisEvaluator(data_dir)
        
    def run_comparison(self, num_cot_examples: int = 2, max_test_cases: Optional[int] = None) -> Dict[str, Any]:
        """è¿è¡Œå¯¹æ¯”è¯„ä¼°"""
        logger.info("="*60)
        logger.info("å¼€å§‹å¯¹æ¯”è¯„ä¼°ï¼šåŸç‰ˆ vs CoTå¢å¼ºç‰ˆ")
        logger.info("="*60)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = AzureModel()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.original_evaluator.load_test_data()
        if not test_data:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ•°æ®")
            return {}
        
        # é™åˆ¶æµ‹è¯•æ¡ˆä¾‹æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        if max_test_cases:
            test_data = test_data[:max_test_cases]
            logger.info(f"é™åˆ¶æµ‹è¯•æ¡ˆä¾‹æ•°é‡ä¸º: {max_test_cases}")
        
        logger.info(f"å°†è¯„ä¼° {len(test_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        
        # ç»Ÿè®¡æµ‹è¯•æ•°æ®åˆ†å¸ƒ
        class_counts = {}
        for case in test_data:
            label = case['true_label']
            class_counts[label] = class_counts.get(label, 0) + 1
        
        print("\næµ‹è¯•æ•°æ®åˆ†å¸ƒ:")
        disease_names = {
            'GIST': 'èƒƒè‚ é—´è´¨ç˜¤',
            'LEIOMYOMA': 'å¹³æ»‘è‚Œç˜¤',
            'PANCREATIC': 'å¼‚ä½èƒ°è…º', 
            'SCHWANNOMA': 'ç¥ç»é˜ç˜¤',
            'OTHER': 'å…¶ä»–'
        }
        for label, count in class_counts.items():
            name = disease_names.get(label, label)
            print(f"  {name}: {count} ä¾‹")
        
        # æ£€æŸ¥CoTç¤ºä¾‹æ˜¯å¦å¯ç”¨
        if not self.cot_evaluator.cot_examples:
            logger.error("æ²¡æœ‰åŠ è½½åˆ°CoTç¤ºä¾‹ï¼Œæ— æ³•è¿›è¡ŒCoTå¢å¼ºè¯„ä¼°")
            return {}
        
        print(f"\nCoTç¤ºä¾‹åˆ†å¸ƒ:")
        total_cot_examples = sum(len(examples) for examples in self.cot_evaluator.cot_examples.values())
        print(f"  æ€»CoTç¤ºä¾‹æ•°: {total_cot_examples}")
        for category, examples in self.cot_evaluator.cot_examples.items():
            if examples:
                name = disease_names.get(category, category)
                print(f"  {name}: {len(examples)} ä¸ªç¤ºä¾‹")
        
        # è¿è¡ŒåŸç‰ˆè¯„ä¼°
        logger.info("\n" + "="*60)
        logger.info("1. è¿è¡ŒåŸç‰ˆè¯„ä¼°ï¼ˆæ— CoTä¸Šä¸‹æ–‡ï¼‰")
        logger.info("="*60)
        
        try:
            original_result = self.original_evaluator.evaluate_model(model, test_data)
            logger.info("âœ… åŸç‰ˆè¯„ä¼°å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ åŸç‰ˆè¯„ä¼°å¤±è´¥: {e}")
            return {}
        
        # è¿è¡ŒCoTå¢å¼ºè¯„ä¼°
        logger.info("\n" + "="*60)
        logger.info(f"2. è¿è¡ŒCoTå¢å¼ºè¯„ä¼°ï¼ˆæ¯æ¡ˆä¾‹{num_cot_examples}ä¸ªç¤ºä¾‹ï¼‰")
        logger.info("="*60)
        
        try:
            cot_result = self.cot_evaluator.evaluate_model_with_cot(model, test_data, num_cot_examples)
            logger.info("âœ… CoTå¢å¼ºè¯„ä¼°å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ CoTå¢å¼ºè¯„ä¼°å¤±è´¥: {e}")
            return {}
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_result = self._generate_comparison_report(original_result, cot_result, num_cot_examples)
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        self._save_comparison_results(comparison_result)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        self._print_comparison_results(comparison_result)
        
        return comparison_result
    
    def _generate_comparison_report(self, original_result: EvaluationResult, 
                                  cot_result: EvaluationResult, 
                                  num_cot_examples: int) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        
        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        accuracy_improvement = cot_result.accuracy - original_result.accuracy
        accuracy_improvement_pct = (accuracy_improvement / original_result.accuracy * 100) if original_result.accuracy > 0 else 0
        
        # è®¡ç®—å„ç±»åˆ«çš„æ”¹è¿›
        category_improvements = {}
        disease_names = {
            'GIST': 'èƒƒè‚ é—´è´¨ç˜¤',
            'LEIOMYOMA': 'å¹³æ»‘è‚Œç˜¤',
            'PANCREATIC': 'å¼‚ä½èƒ°è…º',
            'SCHWANNOMA': 'ç¥ç»é˜ç˜¤',
            'OTHER': 'å…¶ä»–'
        }
        
        for category in original_result.per_class_metrics.keys():
            original_metrics = original_result.per_class_metrics[category]
            cot_metrics = cot_result.per_class_metrics[category]
            
            category_improvements[category] = {
                'precision_improvement': cot_metrics['precision'] - original_metrics['precision'],
                'recall_improvement': cot_metrics['recall'] - original_metrics['recall'],
                'f1_improvement': cot_metrics['f1'] - original_metrics['f1'],
                'original_f1': original_metrics['f1'],
                'cot_f1': cot_metrics['f1']
            }
        
        comparison_result = {
            'evaluation_time': datetime.now().isoformat(),
            'test_cases_count': original_result.total_cases,
            'cot_examples_per_case': num_cot_examples,
            'total_cot_examples_used': cot_result.cot_examples_used,
            
            'original_results': {
                'accuracy': original_result.accuracy,
                'correct_predictions': original_result.correct_predictions,
                'execution_time': original_result.execution_time
            },
            
            'cot_results': {
                'accuracy': cot_result.accuracy,
                'correct_predictions': cot_result.correct_predictions,
                'execution_time': cot_result.execution_time
            },
            
            'improvements': {
                'accuracy_improvement': accuracy_improvement,
                'accuracy_improvement_percentage': accuracy_improvement_pct,
                'additional_correct_predictions': cot_result.correct_predictions - original_result.correct_predictions,
                'execution_time_increase': cot_result.execution_time - original_result.execution_time
            },
            
            'category_wise_comparison': {}
        }
        
        # æ·»åŠ å„ç±»åˆ«å¯¹æ¯”
        for category, improvements in category_improvements.items():
            category_name = disease_names.get(category, category)
            comparison_result['category_wise_comparison'][category_name] = {
                'åŸç‰ˆF1åˆ†æ•°': f"{improvements['original_f1']:.4f}",
                'CoTå¢å¼ºF1åˆ†æ•°': f"{improvements['cot_f1']:.4f}",
                'F1æå‡': f"{improvements['f1_improvement']:.4f}",
                'ç²¾ç¡®ç‡æå‡': f"{improvements['precision_improvement']:.4f}",
                'å¬å›ç‡æå‡': f"{improvements['recall_improvement']:.4f}"
            }
        
        return comparison_result
    
    def _save_comparison_results(self, comparison_result: Dict[str, Any]):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç¡®ä¿resultsç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
        comparison_file = f"results/evaluation_comparison_{timestamp}.json"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_result, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç®€åŒ–çš„CSVæŠ¥å‘Š
        csv_data = {
            'è¯„ä¼°æ–¹æ³•': ['åŸç‰ˆè¯„ä¼°', 'CoTå¢å¼ºè¯„ä¼°'],
            'æ€»ä½“å‡†ç¡®ç‡': [
                f"{comparison_result['original_results']['accuracy']:.4f}",
                f"{comparison_result['cot_results']['accuracy']:.4f}"
            ],
            'æ­£ç¡®é¢„æµ‹æ•°': [
                comparison_result['original_results']['correct_predictions'],
                comparison_result['cot_results']['correct_predictions']
            ],
            'æ‰§è¡Œæ—¶é—´(ç§’)': [
                f"{comparison_result['original_results']['execution_time']:.2f}",
                f"{comparison_result['cot_results']['execution_time']:.2f}"
            ]
        }
        
        csv_df = pd.DataFrame(csv_data)
        csv_file = f"results/evaluation_comparison_summary_{timestamp}.csv"
        csv_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {comparison_file}")
        logger.info(f"å¯¹æ¯”æ‘˜è¦å·²ä¿å­˜åˆ°: {csv_file}")
    
    def _print_comparison_results(self, comparison_result: Dict[str, Any]):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æµ‹è¯•æ¡ˆä¾‹æ•°: {comparison_result['test_cases_count']}")
        print(f"æ¯æ¡ˆä¾‹CoTç¤ºä¾‹æ•°: {comparison_result['cot_examples_per_case']}")
        print(f"æ€»CoTç¤ºä¾‹ä½¿ç”¨æ•°: {comparison_result['total_cot_examples_used']}")
        
        print(f"\n{'æ–¹æ³•':<15} {'å‡†ç¡®ç‡':<10} {'æ­£ç¡®æ•°':<8} {'æ‰§è¡Œæ—¶é—´(ç§’)'}")
        print("-" * 50)
        print(f"{'åŸç‰ˆè¯„ä¼°':<15} {comparison_result['original_results']['accuracy']:<10.4f} "
              f"{comparison_result['original_results']['correct_predictions']:<8} "
              f"{comparison_result['original_results']['execution_time']:.2f}")
        print(f"{'CoTå¢å¼ºè¯„ä¼°':<15} {comparison_result['cot_results']['accuracy']:<10.4f} "
              f"{comparison_result['cot_results']['correct_predictions']:<8} "
              f"{comparison_result['cot_results']['execution_time']:.2f}")
        
        # æ”¹è¿›æŒ‡æ ‡
        improvements = comparison_result['improvements']
        print(f"\nğŸš€ æ”¹è¿›æ•ˆæœ:")
        print(f"  å‡†ç¡®ç‡æå‡: {improvements['accuracy_improvement']:.4f} "
              f"({improvements['accuracy_improvement_percentage']:+.2f}%)")
        print(f"  é¢å¤–æ­£ç¡®é¢„æµ‹: {improvements['additional_correct_predictions']} ä¸ª")
        print(f"  æ‰§è¡Œæ—¶é—´å¢åŠ : {improvements['execution_time_increase']:.2f} ç§’")
        
        # å„ç±»åˆ«å¯¹æ¯”
        print(f"\nğŸ“ˆ å„ç—…ç§F1åˆ†æ•°å¯¹æ¯”:")
        print("-" * 60)
        print(f"{'ç—…ç§':<12} {'åŸç‰ˆF1':<10} {'CoT F1':<10} {'æå‡'}")
        print("-" * 60)
        
        for category, metrics in comparison_result['category_wise_comparison'].items():
            print(f"{category:<12} {metrics['åŸç‰ˆF1åˆ†æ•°']:<10} {metrics['CoTå¢å¼ºF1åˆ†æ•°']:<10} {metrics['F1æå‡']}")
        
        # ç»“è®º
        if improvements['accuracy_improvement'] > 0:
            print(f"\nâœ… ç»“è®º: CoTä¸Šä¸‹æ–‡æ˜¾è‘—æå‡äº†æ¨¡å‹è¯Šæ–­å‡†ç¡®ç‡!")
        elif improvements['accuracy_improvement'] == 0:
            print(f"\nâ– ç»“è®º: CoTä¸Šä¸‹æ–‡å¯¹å‡†ç¡®ç‡æ— æ˜æ˜¾å½±å“")
        else:
            print(f"\nâŒ ç»“è®º: CoTä¸Šä¸‹æ–‡ç•¥å¾®é™ä½äº†å‡†ç¡®ç‡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç¤ºä¾‹æ•°é‡æˆ–è´¨é‡")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ” å¤§æ¨¡å‹è¯Šæ–­å‡†ç¡®ç‡å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ")
    print("æ¯”è¾ƒåŸç‰ˆè¯„ä¼° vs CoTå¢å¼ºè¯„ä¼°")
    print("="*60)
    
    # åˆ›å»ºå¯¹æ¯”è¯„ä¼°å™¨
    comparator = ComparisonEvaluator()
    
    # è¿è¡Œå¯¹æ¯”è¯„ä¼°
    # å¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š
    # - num_cot_examples: æ¯ä¸ªæµ‹è¯•æ¡ˆä¾‹ä½¿ç”¨çš„CoTç¤ºä¾‹æ•°é‡
    # - max_test_cases: é™åˆ¶æµ‹è¯•æ¡ˆä¾‹æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
    try:
        comparison_result = comparator.run_comparison(
            num_cot_examples=2,      # æ¯æ¡ˆä¾‹ä½¿ç”¨2ä¸ªCoTç¤ºä¾‹
            max_test_cases=None      # ä½¿ç”¨æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ï¼Œå¯ä»¥è®¾ç½®ä¸ºå°æ•°å­—è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        )
        
        if comparison_result:
            print(f"\nğŸ‰ å¯¹æ¯”è¯„ä¼°å®Œæˆï¼")
            print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
        
    except Exception as e:
        logger.error(f"å¯¹æ¯”è¯„ä¼°å¤±è´¥: {e}")

if __name__ == "__main__":
    main() 